{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b846e9e-b9a6-4e26-9246-f9aabe62b7d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd7f7f3e-387c-4857-8167-cdcb660ee9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import hazm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41553641-60fe-47e7-b388-3563db8cc0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2896979/1730290655.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dataset = pd.read_csv(file_path, delimiter='  ', skip_blank_lines=False)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'All.conll'\n",
    "\n",
    "dataset = pd.read_csv(file_path, delimiter='  ', skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb34eb96-3850-4dea-a67f-e70a1734fe16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>۵</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طبقه</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>۳</td>\n",
       "      <td>B-Attributes of the property (A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>واحدی</td>\n",
       "      <td>I-Attributes of the property (A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>طبقه</td>\n",
       "      <td>B-Attributes of the property (A)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word                               tag\n",
       "0      ۵                                 O\n",
       "1   طبقه                                 O\n",
       "2      ۳  B-Attributes of the property (A)\n",
       "3  واحدی  I-Attributes of the property (A)\n",
       "4   طبقه  B-Attributes of the property (A)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51780eb-2323-43da-a98a-940c8467adef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentence = []\n",
    "list_tags = []\n",
    "list_tag = []\n",
    "for index, row in dataset.iterrows():\n",
    "    if pd.isna(row['word']):\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        list_tags.append(list_tag)\n",
    "        list_tag = []\n",
    "        pass\n",
    "    else:\n",
    "        sentence.append(row['word'])\n",
    "        list_tag.append(row['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1035caa9-ee3d-4266-855e-464faa2bab88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "558\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2985dc-ad6f-4407-9397-31f10edf5e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the mapping from strings to numbers\n",
    "mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-Locality (L)\": 1,\n",
    "    \"I-Locality (L)\": 2,\n",
    "    \"B-Total Price (P)\": 3,\n",
    "    \"I-Total Price (P)\": 4,\n",
    "    \"B-Land Area (LA)\": 5,\n",
    "    \"I-Land Area (LA)\": 6,\n",
    "    \"B-Cost per land area (C)\": 7,\n",
    "    \"I-Cost per land area (C)\": 8,\n",
    "    \"B-Contact name (N)\": 9,\n",
    "    \"I-Contact name (N)\": 10,\n",
    "    \"B-Contact telephone (T)\": 11,\n",
    "    \"I-Contact telephone (T)\": 12,\n",
    "    \"B-Attributes of the property (A)\": 13,\n",
    "    \"I-Attributes of the property (A)\": 14\n",
    "}\n",
    "\n",
    "# Convert strings to numbers based on the mapping\n",
    "list_tags = [[mapping[item] for item in list_tag] for list_tag in list_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dbe6798-31bb-4ee0-bc78-0c183fd67e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[۵, طبقه, ۳, واحدی, طبقه, دوم, ۲, خوابه, ۲, سر...</td>\n",
       "      <td>[0, 0, 13, 14, 13, 14, 13, 14, 13, 14, 14, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[میدان, ولیعصر, چهار, پارکینگ, ۲۵۵, متر, زمین,...</td>\n",
       "      <td>[1, 2, 13, 14, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[بهترین, خرید, و, سرمایه, گذاری, سال, فرعی, نف...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 13, 14, 13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[۴۳۰, مترمستغلات, سند, اداری, دسترسی, عالی, به...</td>\n",
       "      <td>[5, 6, 13, 14, 13, 14, 14, 14, 14, 14, 14, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[دید, و, نور, عالی, خیابان, اصلی, بسیار, خوش, ...</td>\n",
       "      <td>[13, 14, 14, 14, 1, 2, 0, 13, 14, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [۵, طبقه, ۳, واحدی, طبقه, دوم, ۲, خوابه, ۲, سر...   \n",
       "1  [میدان, ولیعصر, چهار, پارکینگ, ۲۵۵, متر, زمین,...   \n",
       "2  [بهترین, خرید, و, سرمایه, گذاری, سال, فرعی, نف...   \n",
       "3  [۴۳۰, مترمستغلات, سند, اداری, دسترسی, عالی, به...   \n",
       "4  [دید, و, نور, عالی, خیابان, اصلی, بسیار, خوش, ...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 13, 14, 13, 14, 13, 14, 13, 14, 14, 13,...  \n",
       "1  [1, 2, 13, 14, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 13, 14, 13, ...  \n",
       "3  [5, 6, 13, 14, 13, 14, 14, 14, 14, 14, 14, 13,...  \n",
       "4  [13, 14, 14, 14, 1, 2, 0, 13, 14, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'tokens': sentences, 'ner_tags': list_tags})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953d8336-5832-40f3-b7f8-895b523aa46e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 446\n",
      "Test set size: 56\n",
      "Validation set size: 56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the DataFrame into train, test, and validation sets\n",
    "train_df, test_val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Reset the index of the DataFrames\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(\"Train set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))\n",
    "print(\"Validation set size:\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "489a10c6-ca3d-4a04-b0ee-42eb9011a342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "datasets_train_test = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"test\": Dataset.from_pandas(test_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e9587e-eccf-4f4a-bfaf-64eb9df53e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 446\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ea3b7-90b9-4f62-9431-8a6bfae63865",
   "metadata": {
    "tags": []
   },
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d3b46b8-59c1-487b-9bbb-f6ef11c1c493",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بزرگ\\u200cترین',\n",
       " 'شبکه',\n",
       " 'فروش',\n",
       " 'و',\n",
       " 'بروز\\u200cترین',\n",
       " 'سامانه',\n",
       " 'فایلینگ',\n",
       " 'ملک',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'با',\n",
       " 'بیش',\n",
       " 'از',\n",
       " '۶۰',\n",
       " 'کارشناس',\n",
       " 'فعال',\n",
       " 'در',\n",
       " 'سطح',\n",
       " 'تبریز',\n",
       " 'شخصی',\n",
       " 'ساز',\n",
       " 'تک',\n",
       " 'واحده',\n",
       " 'دو',\n",
       " 'انباری',\n",
       " 'برای',\n",
       " 'هر',\n",
       " 'واحد',\n",
       " 'مستر',\n",
       " 'دار',\n",
       " 'نما',\n",
       " 'و',\n",
       " 'مشاعات',\n",
       " 'شیک',\n",
       " 'آسانسور',\n",
       " '۶',\n",
       " 'نفره',\n",
       " 'پارکینگ',\n",
       " 'اختصاصی',\n",
       " 'پوشش',\n",
       " 'کف',\n",
       " 'سرامیک',\n",
       " 'آماده',\n",
       " 'تحویل',\n",
       " 'فروشنده',\n",
       " 'واقعی',\n",
       " 'جهت',\n",
       " 'کسب',\n",
       " 'اطلاعات',\n",
       " 'بیشتر',\n",
       " 'تماس',\n",
       " 'بگیرید',\n",
       " 'کارشناس',\n",
       " 'منطقه',\n",
       " 'اسعدی',\n",
       " 'از',\n",
       " 'دادن',\n",
       " 'آدرس',\n",
       " 'دقیق',\n",
       " 'واحد',\n",
       " 'به',\n",
       " 'صورت',\n",
       " 'تلفنی',\n",
       " 'معذوریم',\n",
       " 'آدرس',\n",
       " 'دفتر',\n",
       " 'چهارراه',\n",
       " 'منصور',\n",
       " 'پشت',\n",
       " 'برج',\n",
       " 'ابریشم']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = datasets_train_test\n",
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c4e838-ab96-4809-a50a-83bfb5b89302",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ec1211-badf-4260-a2c9-eaa6b60d1776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_names = [\"O\", \"B-Locality (L)\", \"I-Locality (L)\",\n",
    "    \"B-Total Price (P)\", \"I-Total Price (P)\", \"B-Land Area (LA)\", \"I-Land Area (LA)\",\n",
    "    \"B-Cost per land area (C)\", \"I-Cost per land area (C)\", \"B-Contact name (N)\",\n",
    "    \"I-Contact name (N)\", \"B-Contact telephone (T)\", \"I-Contact telephone (T)\", \n",
    "    \"B-Attributes of the property (A)\", \"I-Attributes of the property (A)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fbfa8a8-0479-4df3-9581-e8d214db5b30",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بزرگ‌ترین شبکه فروش و بروز‌ترین سامانه فایلینگ ملک در تبریز          با بیش از ۶۰ کارشناس فعال در سطح تبریز          شخصی ساز تک واحده دو انباری برای هر واحد مستر                             دار                              نما                              و                                مشاعات                           شیک                              آسانسور                          ۶ نفره پارکینگ                          اختصاصی پوشش کف                               سرامیک                           آماده تحویل فروشنده واقعی جهت کسب اطلاعات بیشتر تماس                    بگیرید                  کارشناس            منطقه              اسعدی              از دادن آدرس دقیق واحد به صورت تلفنی معذوریم آدرس           دفتر           چهارراه        منصور          پشت            برج            ابریشم         \n",
      "O         O    O    O O         O      O       O   O  B-Locality (L) O  O   O  O  O       O    O  O   B-Locality (L) O    O   O  O     O  O      O    O  O    B-Attributes of the property (A) I-Attributes of the property (A) B-Attributes of the property (A) I-Attributes of the property (A) I-Attributes of the property (A) I-Attributes of the property (A) B-Attributes of the property (A) O O    B-Attributes of the property (A) O       O    B-Attributes of the property (A) I-Attributes of the property (A) O     O     O       O     O   O   O       O     B-Contact telephone (T) I-Contact telephone (T) B-Contact name (N) I-Contact name (N) I-Contact name (N) O  O    O    O    O    O  O    O     O       B-Locality (L) I-Locality (L) I-Locality (L) I-Locality (L) I-Locality (L) I-Locality (L) I-Locality (L) \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9930e-da4d-4662-a91a-70b85353953b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7ec517-1850-4e21-8ae1-ff17e8561e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"HooshvareLab/bert-fa-zwnj-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84565720-5abe-48d8-81ae-5223e09b1a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b9a285d-ca77-4ee7-bfd0-7b19f43bfbf7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'بزرگ',\n",
       " '[ZWNJ]',\n",
       " 'ترین',\n",
       " 'شبکه',\n",
       " 'فروش',\n",
       " 'و',\n",
       " 'بروز',\n",
       " '[ZWNJ]',\n",
       " 'ترین',\n",
       " 'سامانه',\n",
       " 'فایلی',\n",
       " '##نگ',\n",
       " 'ملک',\n",
       " 'در',\n",
       " 'تبریز',\n",
       " 'با',\n",
       " 'بیش',\n",
       " 'از',\n",
       " '۶۰',\n",
       " 'کارشناس',\n",
       " 'فعال',\n",
       " 'در',\n",
       " 'سطح',\n",
       " 'تبریز',\n",
       " 'شخصی',\n",
       " 'ساز',\n",
       " 'تک',\n",
       " 'واحده',\n",
       " 'دو',\n",
       " 'انباری',\n",
       " 'برای',\n",
       " 'هر',\n",
       " 'واحد',\n",
       " 'مستر',\n",
       " 'دار',\n",
       " 'نما',\n",
       " 'و',\n",
       " 'مشاع',\n",
       " '##ات',\n",
       " 'شیک',\n",
       " 'آ',\n",
       " '##سانس',\n",
       " '##ور',\n",
       " '۶',\n",
       " 'نفره',\n",
       " 'پارکینگ',\n",
       " 'اختصاصی',\n",
       " 'پوشش',\n",
       " 'کف',\n",
       " 'سرامیک',\n",
       " 'آ',\n",
       " '##ماده',\n",
       " 'تحویل',\n",
       " 'فروشنده',\n",
       " 'واقعی',\n",
       " 'جهت',\n",
       " 'کسب',\n",
       " 'اطلاعات',\n",
       " 'بیشتر',\n",
       " 'تماس',\n",
       " 'بگیرید',\n",
       " 'کارشناس',\n",
       " 'منطقه',\n",
       " 'اسعدی',\n",
       " 'از',\n",
       " 'دادن',\n",
       " 'آ',\n",
       " '##در',\n",
       " '##س',\n",
       " 'دقیق',\n",
       " 'واحد',\n",
       " 'به',\n",
       " 'صورت',\n",
       " 'تلفنی',\n",
       " 'معذ',\n",
       " '##وریم',\n",
       " 'آ',\n",
       " '##در',\n",
       " '##س',\n",
       " 'دفتر',\n",
       " 'چهارراه',\n",
       " 'منصور',\n",
       " 'پشت',\n",
       " 'برج',\n",
       " 'ابریشم',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f237120-d752-4951-87fb-0a50353af725",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa445500-228b-4ba2-98b2-b189736ff069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb29bce8-a119-4d74-a675-d7afce52601b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 13, 14, 14, 14, 13, 0, 0, 13, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 13, 14, 14, 14, 14, 13, 14, 14, 0, 0, 13, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4a6ac0f-c17a-498d-b8c5-16450ec9c3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a655fb1-ad43-415e-88d2-35bb79971593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b1665bf-8965-41b7-b8cd-06cb30f2dea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7649a7-b1df-47f5-a2e8-82bd1f00c824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            1,    0,    0,    0,    0,    0,    0,    0,    0,    0,   13,   14,\n",
       "           13,   14,   14,   14,   14,   13,   14,   14,    0,    0,   13,    0,\n",
       "            0,   13,   14,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           11,   12,    9,   10,   10,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    1,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    1,    2,    2,    2,    2,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   13,   14,\n",
       "           14,   14,   14,   14,   14,    0,    0,    0,   13,   14,   14,   14,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    9,   10,    0,    0,    0,    0,    0,\n",
       "            0,    0,   11,   12,   12,    9,   10,   10,    1,    2,    0,    0,\n",
       "            0,    0, -100]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00eda12e-ee80-41a5-8ec6-294c4e4a9b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 13, 14, 14, 14, 14, 13, 14, 14, 0, 0, 13, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 9, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 14, 14, 14, 0, 0, 0, 13, 14, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 9, 10, 10, 1, 2, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8008ef5b-adda-465b-b66d-44f8189003e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/user01/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--seqeval/541ae017dc683f85116597d48f621abc7b21b88dc42ec937c71af5415f0af63c (last modified on Mon Jul  3 23:26:31 2023) since it couldn't be found locally at evaluate-metric--seqeval, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b54647-1a95-4cb2-b46d-980bc86f0e7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Locality (L)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Locality (L)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Attributes of the property (A)',\n",
       " 'I-Attributes of the property (A)',\n",
       " 'B-Attributes of the property (A)',\n",
       " 'I-Attributes of the property (A)',\n",
       " 'I-Attributes of the property (A)',\n",
       " 'I-Attributes of the property (A)',\n",
       " 'B-Attributes of the property (A)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Attributes of the property (A)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Attributes of the property (A)',\n",
       " 'I-Attributes of the property (A)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Contact telephone (T)',\n",
       " 'I-Contact telephone (T)',\n",
       " 'B-Contact name (N)',\n",
       " 'I-Contact name (N)',\n",
       " 'I-Contact name (N)',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Locality (L)',\n",
       " 'I-Locality (L)',\n",
       " 'I-Locality (L)',\n",
       " 'I-Locality (L)',\n",
       " 'I-Locality (L)',\n",
       " 'I-Locality (L)',\n",
       " 'I-Locality (L)']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2560a0d-cf6b-4532-8913-f6e634d2f924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Attributes of the property (A)': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'number': 5},\n",
       " 'Contact name (N)': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'Contact telephone (T)': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'number': 1},\n",
       " 'Locality (L)': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9591dfd-bc0b-4a4d-96e2-fa40a5ef8a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d53c6051-1ace-4222-8cca-3654bab27560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c54c6b9a-b26c-4630-bb91-c836583e0f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-fa-zwnj-base were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5d54bee-d6a1-4a2b-9627-b5b4291050c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9f0f7ed-fe4f-4b09-9c06-c2836cf74ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ParsBERT_V3_ner_results\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_steps=50,\n",
    "    logging_dir='ParsBERT_V3_ner_logs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2dab204f-6865-4d8f-b64a-36c25650aa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45fd68dc-e0f8-43d6-8231-750ef043f4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/envs/mehrab2/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 36:24, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.406900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=560, training_loss=0.31805915662220546, metrics={'train_runtime': 2189.194, 'train_samples_per_second': 4.075, 'train_steps_per_second': 0.256, 'total_flos': 837345267265500.0, 'train_loss': 0.31805915662220546, 'epoch': 20.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2624cfc1-3458-458f-bbf9-dd9fd86a543e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8843041658401489,\n",
       " 'eval_precision': 0.40850277264325324,\n",
       " 'eval_recall': 0.4682203389830508,\n",
       " 'eval_f1': 0.4363277393879565,\n",
       " 'eval_accuracy': 0.8177975058127246,\n",
       " 'eval_runtime': 5.1568,\n",
       " 'eval_samples_per_second': 10.86,\n",
       " 'eval_steps_per_second': 0.776,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cc119-75c7-4858-8a68-31c99f716fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
