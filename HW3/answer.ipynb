{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the output activation volume dimensions and number of parameters for the first convolutional layer, we can use the following formulas:\n",
    "\n",
    "\n",
    "Output spatial dimensions:\n",
    "\n",
    "\n",
    "$H_{out} =\\frac{(H_in + 2 * padding - filter_size)}{stride + 1}$\n",
    "\n",
    "\n",
    "$W_{out} =\\frac{(W_in + 2 * padding - filter_size)}{ stride + 1}$\n",
    "\n",
    "\n",
    "Number of parameters:\n",
    "\n",
    "\n",
    "$num_params = (filter_size * filter_size * input_channels + 1) * num_filters$\n",
    "\n",
    "\n",
    "\n",
    "Given the following information:\n",
    "\n",
    "\n",
    "Input shape: 128 × 128 (H_in = 128, W_in = 128)\n",
    "\n",
    "\n",
    "Stride: 1\n",
    "\n",
    "\n",
    "Filter size: 5 × 5 (filter_size = 5)\n",
    "\n",
    "\n",
    "Input padding: 2 (padding = 2)\n",
    "\n",
    "\n",
    "Number of filters: 16 (num_filters = 16)\n",
    "\n",
    "\n",
    "Let's compute the output activation volume dimensions first:\n",
    "\n",
    "\n",
    "$H_{out} = \\frac{(128 + 2 * 2 - 5) }{ 1 + 1} = 128$\n",
    "\n",
    "\n",
    "$W_{out} = \\frac{(128 + 2 * 2 - 5) }{ 1 + 1} = 128$\n",
    "\n",
    "\n",
    "Therefore, the output activation volume dimensions are (128, 128, 16) since we have 16 filters.\n",
    "Now, let's compute the number of parameters:\n",
    "\n",
    "\n",
    "$numberofparameters=(5×5×3+1)×16=1216$\n",
    "\n",
    "\n",
    "In this case, the filter size is 5 × 5, the input channels are 3 (assuming RGB input), and the number of filters is 16\n",
    "\n",
    "\n",
    "Therefore, the first convolutional layer has an output activation volume with dimensions (128, 128, 16) and a total of 416 parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer:\n",
    "\n",
    "Stride: 1\n",
    "Filter size: 5 × 5\n",
    "Input padding: 2\n",
    "Number of filters: 16\n",
    "\n",
    "\n",
    "Pooling layer:\n",
    "\n",
    "\n",
    "Stride: 2\n",
    "Width/height: 2\n",
    "Let's start with the input image shape:\n",
    "\n",
    "\n",
    "Input shape: 128 × 128 × 3 (assuming the input has 3 channels\n",
    ")\n",
    "\n",
    "Module 1:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "\n",
    "Input shape: 128 × 128 × 3\n",
    "Output activation volume dimensions: (128, 128, 16)\n",
    "Number of parameters: (5 × 5 × 3 + 1) × 16 = 1216\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "\n",
    "Input shape: (128, 128, 16)\n",
    "Output activation volume dimensions: (64, 64, 16) (after applying the pooling operation)\n",
    "Number of parameters: 0 (pooling layer does not have trainable parameters)\n",
    "\n",
    "\n",
    "Module 2:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "Input shape: (64, 64, 16)\n",
    "Output activation volume dimensions: (64, 64, 16)\n",
    "Number of parameters: (5 × 5 × 16 + 1) × 16 = 6416\n",
    "\n",
    "\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "Input shape: (64, 64, 16)\n",
    "Output activation volume dimensions: (32, 32, 16)\n",
    "Number of parameters: 0\n",
    "\n",
    "\n",
    "Module 3:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "Input shape: (32, 32, 16)\n",
    "Output activation volume dimensions: (32, 32, 16)\n",
    "Number of parameters: (5 × 5 × 16 + 1) × 16 = 6416\n",
    "\n",
    "\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "Input shape: (32, 32, 16)\n",
    "Output activation volume dimensions: (16, 16, 16)\n",
    "Number of parameters: 0\n",
    "\n",
    "\n",
    "Module 4:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "Input shape: (16, 16, 16)\n",
    "Output activation volume dimensions: (16, 16, 16)\n",
    "Number of parameters: (5 × 5 × 16 + 1) × 16 = 6416\n",
    "\n",
    "\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "Input shape: (16, 16, 16)\n",
    "Output activation volume dimensions: (8, 8, 16)\n",
    "Number of parameters: 0\n",
    "\n",
    "\n",
    "Module 5:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "Input shape: (8, 8, 16)\n",
    "Output activation volume dimensions: (8, 8, 16)\n",
    "Number of parameters: (5 × 5 × 16 + 1) × 16 = 6416\n",
    "\n",
    "\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "Input shape: (8, 8, 16)\n",
    "Output activation volume dimensions: (4, 4, 16)\n",
    "Number of parameters: 0\n",
    "\n",
    "\n",
    "Module 6:\n",
    "\n",
    "\n",
    "Convolutional layer:\n",
    "\n",
    "\n",
    "Input shape: (4, 4, 16)\n",
    "Output activation volume dimensions: (4, 4, 16)\n",
    "Number of parameters: (5 × 5 × 16 + 1) × 16 = 6416\n",
    "\n",
    "\n",
    "MaxPooling layer:\n",
    "\n",
    "\n",
    "Input shape: (4, 4, 16)\n",
    "Output activation volume dimensions: (2, 2, 16)\n",
    "Number of parameters: 0\n",
    "\n",
    "\n",
    "After passing the input through the entire network, the final output activation volume dimensions will be:\n",
    "\n",
    "\n",
    "(2, 2, 16)\n",
    "\n",
    "\n",
    "The number of parameters in the entire network can be calculated by summing up the number of parameters in each module:\n",
    "\n",
    "\n",
    "Total number of parameters = 1216 + 6416 + 6416 + 6416 + 6416 + 6416 = 33,696\n",
    "\n",
    "\n",
    "Therefore, the output activation volume dimensions after passing the input through the entire network are (2, 2, 16), and the total number of parameters in the network is 33,696."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the information from the previous calculations:\n",
    "\n",
    "\n",
    "Number of parameters in Conv-Pool-ReLU modules: 33,696\n",
    "\n",
    "\n",
    "To determine the number of parameters in the FullyConnected layer, we need to know the number of units (neurons) in that layer and the size of the previous layer's output.\n",
    "\n",
    "\n",
    "Assuming the output activation volume dimensions after the last Conv-Pool-ReLU module are (2, 2, 16), we need to flatten it before connecting to the FullyConnected layer. The flattened output will have a shape of (2 * 2 * 16) = 64.\n",
    "\n",
    "\n",
    "Let's say the FullyConnected layer has n units. The number of parameters in the FullyConnected layer is calculated as:\n",
    "\n",
    "\n",
    "Number of parameters = (previous_layer_size + 1) * n\n",
    "\n",
    "\n",
    "Here, \"+ 1\" accounts for the bias term associated with each unit.\n",
    "\n",
    "\n",
    "If we choose n to be a certain number, let's say 128, the number of parameters in the FullyConnected layer will be:\n",
    "\n",
    "\n",
    "Number of parameters = (64 + 1) * 128 = 8,320\n",
    "\n",
    "\n",
    "Adding this to the number of parameters in the Conv-Pool-ReLU modules, we have:\n",
    "\n",
    "\n",
    "Total number of parameters = 33,696 + 8,320 = 42,016\n",
    "\n",
    "\n",
    "Therefore, the total number of parameters in the entire network, including the FullyConnected layer, is 42,016."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) are two popular deep learning models for image generation. Here are some comparisons of the two models, as well as their advantages and limitations:\n",
    "VAEs\n",
    "VAEs follow an unsupervised approach to training\n",
    "\n",
    ".\n",
    "VAEs are simpler to train than GANs and don't require synchronization between their two components\n",
    "\n",
    ".\n",
    "VAEs are widely used in image generation tasks\n",
    "\n",
    ".\n",
    "VAEs can create more effective classification engines for various tasks\n",
    "\n",
    ".\n",
    "VAEs are better suited for signal processing use cases, such as anomaly detection for predictive maintenance or security analytics applications\n",
    "\n",
    ".\n",
    "VAEs are computationally less expensive than GANs\n",
    "\n",
    ".\n",
    "GANs\n",
    "GANs follow a supervised technique to training\n",
    "\n",
    ".\n",
    "GANs can recognize more complicated insights of the input and generate higher and more detailed plausible data than VAEs\n",
    "\n",
    ".\n",
    "GANs are superior to VAEs in creating valid and realistic images\n",
    "\n",
    ".\n",
    "GANs are typically employed when dealing with any kind of imagery or visual data\n",
    "\n",
    ".\n",
    "GANs are used in more demanding tasks like super-resolution and image-to-image translation\n",
    "\n",
    ".\n",
    "GANs are notoriously difficult to work with and require a lot of data and tuning\n",
    "\n",
    ".\n",
    "GANs have slow convergence during training, which can be improved by using tricks like small batch sizes and skip-layer excitation modules\n",
    "\n",
    ".\n",
    "In the end, VAEs are suitable for tasks that require probabilistic modeling and reconstruction ability, while GANs excel in producing visually appealing and diverse samples.VAEs are simpler to train and computationally less expensive, while GANs can generate more detailed and realistic images. VAEs are better suited for signal processing use cases, while GANs are typically employed when dealing with any kind of imagery or visual data. However, GANs are notoriously difficult to work with and require a lot of data and tuning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " GANs are often known for producing visually appealing and high-fidelity images compared to VAEs. Here's the reasoning behind this observation:\n",
    "\n",
    "Objective Evaluation: When evaluating image quality, metrics such as Inception Score (IS) and Fréchet Inception Distance (FID) are commonly used. These metrics measure the quality and diversity of generated images by comparing them to real images. GANs often achieve higher IS and lower FID scores, indicating better image quality and diversity compared to VAEs.\n",
    "\n",
    "Discriminative Adversarial Training: GANs explicitly train a discriminator network to distinguish between real and generated images. This adversarial training framework encourages the generator network to produce more realistic samples by continuously improving and competing against the discriminator. The discriminator acts as a critical feedback mechanism, pushing the generator to generate images that closely resemble real data. This adversarial nature of GAN training helps in capturing fine details and improving image quality.\n",
    "\n",
    "Lack of Pixel-wise Reconstruction Loss: VAEs use a pixel-wise reconstruction loss to encourage the generated images to be similar to the input data. However, this loss tends to produce blurry outputs and may not capture high-frequency details well. GANs, on the other hand, do not rely on pixel-wise reconstruction but focus on capturing the overall data distribution through the discriminator's feedback. This allows GANs to produce sharper and more realistic images.\n",
    "\n",
    "While GANs generally excel in producing high-quality images, it's worth noting that the choice between VAEs and GANs depends on the specific requirements of the task. VAEs are still valuable for tasks that involve reconstruction, inpainting, or probabilistic modeling, where image quality may not be the primary concern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a GAN to generate octopus images with a small curated dataset, applying image augmentation techniques can be beneficial for improving the performance and diversity of the generated samples. Among the three mentioned augmentation techniques, changing the color of pixels and flipping the left-right axis of the image are more likely to be helpful for generating better octopus images. Here's why:\n",
    "\n",
    "Changing the color of pixels: Octopuses exhibit a wide range of colors and patterns, and incorporating color variations in the training data can help the generator capture the diversity of octopus appearances. By randomly altering the color of pixels during augmentation, the generator can learn to produce octopus images with different hues, tones, and color combinations, enhancing the variety of generated samples.\n",
    "\n",
    "Flipping the left-right axis of the image: Octopuses typically exhibit bilateral symmetry, where one side of the body is a mirror image of the other. By flipping the left-right axis of the image during augmentation, the generator can learn to generate octopus images with both left-facing and right-facing orientations. This augmentation technique encourages the generator to produce symmetrical and balanced octopus images, which aligns with the natural characteristics of octopuses.\n",
    "\n",
    "Adding blur to the image: While blur augmentation can be useful for generalization and reducing overfitting in some scenarios, it may not directly contribute to generating better octopus images. Octopuses are known for their intricate details, textures, and tentacles, and adding blur to the images may inadvertently smooth out these fine features. As a result, the generator might struggle to capture the necessary level of detail and texture, potentially leading to less visually appealing octopus images.\n",
    "\n",
    "In summary, when training a GAN to generate better octopus images with a small dataset, augmenting the training data by changing the color of pixels and flipping the left-right axis of the images can help improve the diversity and realism of the generated samples. These augmentation techniques enable the generator to capture the varied coloration and bilateral symmetry characteristics commonly observed in octopuses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mode collapse is a common problem in Generative Adversarial Networks (GANs) where the generator fails to capture the full diversity of the target data distribution and instead produces a limited set of samples or collapses to a single mode. In mode collapse, the generator focuses on generating a small subset of samples that fool the discriminator, ignoring other modes or variations in the data distribution. This results in a lack of diversity and limited coverage of the target distribution.\n",
    "\n",
    "Mode collapse can occur due to several reasons:\n",
    "\n",
    "Discriminator Overpowering: If the discriminator network becomes too powerful or the generator network is not properly trained, the generator may struggle to produce diverse samples that can fool the discriminator. The discriminator's high discriminative ability can overpower the generator, leading to mode collapse.\n",
    "\n",
    "Imbalance in Network Capacities: If the generator and discriminator networks have significant imbalances in terms of network capacity or learning rate, it can hinder the ability of the generator to learn and explore the full range of the data distribution.\n",
    "\n",
    "Lack of Sufficient Training: Mode collapse can also occur when the GAN is trained for insufficient iterations or with limited training data. Without enough training, the generator might not have the opportunity to explore and cover the full range of the target distribution adequately.\n",
    "\n",
    "To overcome mode collapse, several techniques and strategies have been proposed:\n",
    "\n",
    "Architectural Adjustments:\n",
    "\n",
    "Modifying the network architecture of the generator and discriminator, such as increasing their capacity or introducing additional layers, can help prevent mode collapse and improve diversity.\n",
    "Using normalization techniques like batch normalization or spectral normalization can stabilize the training process and mitigate mode collapse.\n",
    "Improved Training Procedures:\n",
    "\n",
    "Applying regularization techniques like dropout or weight regularization to the generator and discriminator can encourage diversity in the generated samples.\n",
    "Employing alternative training methods like Wasserstein GAN (WGAN) or Least Squares GAN (LSGAN) can help stabilize training and reduce mode collapse.\n",
    "Diversity-Promoting Losses:\n",
    "\n",
    "Adding diversity-promoting loss functions, such as maximum-mean discrepancy (MMD) or feature matching loss, can encourage the generator to cover multiple modes and produce diverse samples.\n",
    "Progressive Training:\n",
    "\n",
    "Gradually increasing the complexity of the GAN model by training it on lower-resolution images first and then transitioning to higher resolutions can aid in preventing mode collapse and improving overall image quality.\n",
    "Ensemble Approaches:\n",
    "\n",
    "Training multiple GAN models with different random initializations and then averaging their outputs can help capture a broader range of modes and enhance the diversity of generated samples.\n",
    "Dataset Augmentation:\n",
    "\n",
    "Augmenting the training dataset with additional samples or applying data augmentation techniques, as discussed earlier, can help introduce more diversity and reduce mode collapse.\n",
    "It's important to note that overcoming mode collapse is an ongoing research topic, and the effectiveness of different techniques may vary depending on the specific dataset and problem domain. Experimentation and careful evaluation are necessary to find the most suitable approaches for mitigating mode collapse in GANs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
