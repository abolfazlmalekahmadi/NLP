{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700e1aac-da80-4492-96c4-497c0ba06f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import torch\n",
    "import json\n",
    "import random \n",
    "import glob\n",
    "from hazm import SentenceTokenizer, WordTokenizer, Normalizer, sent_tokenize\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline, TrainingArguments, Trainer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, DistilBertTokenizer\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cdac81c-776f-4828-8e00-697c47198f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b)0\u001b7\u001b[?47h\u001b[1;24r\u001b[m\u001b[4l\u001b[?1h\u001b=Tue Jun 06 02:23:12 2023\n",
      "╒═════════════════════════════════════════════════════════════════════════════╕\n",
      "│ NVITOP 1.0.0        Driver Version: 460.56        CUDA Driver Version: 11.2 │\n",
      "├───────────────────────────────┬──────────────────────┬──────────────────────┤\n",
      "│ GPU  Name        Persistence-M│ Bus-Id        Disp.A │ Volatile Uncorr. ECC │\n",
      "│ Fan  Temp  Perf  Pwr:Usage/Cap│         Memory-Usage │ GPU-Util  Compute M. │\n",
      "╞═══════════════════════════════╪══════════════════════╪══════════════════════╡\n",
      "│\u001b[32m   0  GeForce RTX 3090    Off  \u001b[0m│\u001b[32m 00000000:0B:00.0 Off \u001b[0m│\u001b[32m                  N/A \u001b[0m│\n",
      "│\u001b[32m 73%   56C    P3    92W / 350W \u001b[0m│\u001b[32m      3MiB / 23.70GiB \u001b[0m│\u001b[32m      0%      Default \u001b[0m│\n",
      "╘═══════════════════════════════╧══════════════════════╧══════════════════════╛\n",
      "\u001b[1m\u001b[36m[ CPU: ██▍ 8.0%                           ]\u001b[0m  \u001b[1m( Load Average:  1.00  0.87  0.69 )\u001b[0m\n",
      "\u001b[1m\u001b[35m[ MEM: ▊ 2.6%                             ]\u001b[0m  \u001b[1m\u001b[34m[ SWP: ▏ 0.0%                     ]\u001b[0m\n",
      "\n",
      "╒══════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Processes:                                                     \u001b[1m\u001b[35muser01\u001b[0m\u001b[1m@\u001b[0m\u001b[1m\u001b[32muser01\u001b[0m │\n",
      "│ GPU     PID      USER  GPU-MEM %SM  %CPU  %MEM  TIME  COMMAND                │\n",
      "╞══════════════════════════════════════════════════════════════════════════════╡\n",
      "│  No running processes found                                                  │\n",
      "╘══════════════════════════════════════════════════════════════════════════════╛\n",
      "\u001b[1m\u001b[31mERROR:\u001b[0m Failed to initialize `curses` (curs_set() returned ERR)\n"
     ]
    }
   ],
   "source": [
    "!nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7268f0c9-9e4c-4395-b943-0c36cfcf380b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset_v2(file_path):\n",
    "    all_paragraphs = []\n",
    "    for file in glob.glob(file_path):\n",
    "        with open(file, 'r') as f:\n",
    "            print(f'====== file_name: {file} =======')\n",
    "            if re.match(r'.*\\.json$', file):\n",
    "                data = json.load(f)\n",
    "                for dict_ in data:\n",
    "                    all_paragraphs += dict_['paragraphs']\n",
    "            elif re.match(r'.*\\.csv$', file):\n",
    "                data = pd.read_csv(f)\n",
    "                all_paragraphs += data['text'].tolist()\n",
    "    print(len(all_paragraphs))\n",
    "    # Extract sentences from paragraphs :\n",
    "    sentences_list = []\n",
    "    for paragraph in tqdm(all_paragraphs, desc = 'Sentences tokenization'):\n",
    "        sentences_list += sent_tokenize(paragraph)\n",
    "    # Normalize + remove extra denotations like :\n",
    "    normalized_sentences_list = []\n",
    "    normalizer = Normalizer()\n",
    "    for sentence in tqdm(sentences_list, desc = 'Normalization'):\n",
    "        normalized_sentence = re.sub('[:,،.<>/!@#$%~{}();»«…“”\"؛؟◊♦–\\*\\+_\\^]', ' ', sentence)\n",
    "        normalized_sentence = normalizer.normalize(normalized_sentence)\n",
    "        normalized_sentences_list.append(normalized_sentence)\n",
    "    print(len(normalized_sentences_list))\n",
    "    with open(\"dataset_sentences_v2\", \"wb\") as fp:\n",
    "                pickle.dump(normalized_sentences_list, fp)\n",
    "                \n",
    "                \n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"rb\") as fp:   \n",
    "        dataset_sentences = pickle.load(fp)\n",
    "    return dataset_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14271f4a-375a-4bd9-b07f-38208138f297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        max_length = 200\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', \n",
    "                                       truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            \n",
    "            # resulting input IDs and attention masks are converted to \n",
    "            # PyTorch tensors and appended to the respective lists.\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    # returns the length of the dataset (i.e., the number of input samples).\n",
    "    def __len__(self):\n",
    "        temp = len(self.input_ids)\n",
    "        return temp\n",
    "\n",
    "    # used to retrieve an item at a specific index. \n",
    "    # It returns the input IDs and attention masks corresponding to that index.\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8909038-e720-4bfd-8ea1-9918f74d80b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== file_name: dataset/namnak-4.json =======\n",
      "====== file_name: dataset/hidoctor-1.json =======\n",
      "====== file_name: dataset/review.csv =======\n",
      "====== file_name: dataset/hidoctor-4.json =======\n",
      "====== file_name: dataset/namnak-3.json =======\n",
      "====== file_name: dataset/namnak-1.json =======\n",
      "====== file_name: dataset/hidoctor-5.json =======\n",
      "====== file_name: dataset/hidoctor-3.json =======\n",
      "====== file_name: dataset/alarabiya_data.csv =======\n",
      "====== file_name: dataset/namnak-5.json =======\n",
      "====== file_name: dataset/namnak-2.json =======\n",
      "====== file_name: dataset/hidoctor-2.json =======\n",
      "60615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences tokenization: 100%|██████████████████████████████████████████████████| 60615/60615 [00:00<00:00, 96207.31it/s]\n",
      "Normalization: 100%|█████████████████████████████████████████████████████████| 110261/110261 [00:07<00:00, 14591.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110261\n"
     ]
    }
   ],
   "source": [
    "data_path = 'dataset/*'\n",
    "make_dataset_v2(data_path)   \n",
    "sentences = load_dataset('dataset_sentences_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d244465-1ed1-466b-a8b7-ec899a7fe6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(25004, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian',  bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model.resize_token_embeddings(len(tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2fb9f5-5f96-424c-adbc-2cbb45e0f4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make dataset\n",
    "dataset_sentences = load_dataset('dataset_sentences_v2')\n",
    "dataset_sentences = random.sample(dataset_sentences, int(len(dataset_sentences)))\n",
    "dataset = GPT2Dataset(dataset_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a9c64d-3220-4872-8f86-68ef79b701f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77182 training samples\n",
      "16540 validation samples\n",
      "16539 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1314' max='23160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1314/23160 15:35 < 4:19:30, 1.40 it/s, Epoch 0.34/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.506700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14573/1379703592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer = Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n\u001b[0;32m---> 20\u001b[0;31m         eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n\u001b[0m\u001b[1;32m     21\u001b[0m                                                               \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                               'labels': torch.stack([f[0] for f in data])}).train()\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         )\n\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                 if (\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2717\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.70 * len(dataset))\n",
    "test_size = int(0.15 * len(dataset))\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "batch_size = 20\n",
    "epochs = 6\n",
    "\n",
    "set_seed(555)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f'{train_size} training samples')\n",
    "print(f'{val_size} validation samples')\n",
    "print(f'{test_size} test samples')\n",
    "\n",
    "# train Model \n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=epochs, logging_steps=50, save_steps=3000,\n",
    "                                  per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size,\n",
    "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')\n",
    "\n",
    "trainer = Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()\n",
    "\n",
    "model.save_pretrained(\"./gpt2_namnak_hidoctor/model_base_on_namnak_hidoctor_dataset_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0956980-1a1c-49c6-99dd-a4463db8ac72",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6514b1-f9ca-4403-8136-d19e5975d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(model, tokenizer, text, kind, num, max_length):\n",
    "    # Encode a text inputs\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer,  pad_token_id=tokenizer.eos_token_id)\n",
    "    if kind == 'greedy':\n",
    "        outputs = generator(text, max_length=max_length, num_return_sequences=num)\n",
    "    elif kind == 'beams':\n",
    "        outputs = generator(text, max_length=max_length, num_beams=5, num_return_sequences=num)\n",
    "    elif kind == 'random_sampling':\n",
    "        outputs = generator(text, max_length=max_length, top_k=0, do_sample=True, temperature=0.7, num_return_sequences=num)\n",
    "    elif kind == 'text_p_sampling':   \n",
    "        outputs = generator(text, max_length=max_length, top_k=0, top_p=0.92, do_sample=True, num_return_sequences=num)\n",
    "\n",
    "    elif kind == 'text_k_sampling':   \n",
    "        outputs = generator(text, max_length=max_length, top_k=40, do_sample=True, num_return_sequences=num)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69484d3c-27da-4f76-b68a-367e5916c413",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae3e457-62cc-488b-ac89-6a54dfa8291c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  احتمال خطر سکته های \n",
      "ساله از\n",
      "درصد بیشتر\n",
      "- قلبی\n",
      "درصد وجود\n",
      "‌های قلبی\n",
      ".................................\n",
      "input :  احتمال خطر سکته های م\n",
      "مغیر قلبی\n",
      "محدب مغزی\n",
      "م ادر\n",
      "مشد ید\n",
      "متواتر پیش\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس \n",
      "و خستگی\n",
      "و بهبود\n",
      "است.\n",
      "و اضطراب\n",
      "و کاهش\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس ن\n",
      "ننشینید.\n",
      "ننوشیدن آب\n",
      "ننوشیدن آب\n",
      "ننوشیدن آب\n",
      "ننوشیدن آب\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس نوشیدن \n",
      "آب کافی\n",
      "آب سالم\n",
      "آب همراه\n",
      "یک لیوان\n",
      "این مایعات\n",
      ".................................\n"
     ]
    }
   ],
   "source": [
    "tokenizer_base = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model_base = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "samples = [ 'احتمال خطر سکته های ', \n",
    "         'احتمال خطر سکته های م',\n",
    "         'بهترین روش برای غلبه بر استرس ',\n",
    "         'بهترین روش برای غلبه بر استرس ن',\n",
    "         'بهترین روش برای غلبه بر استرس نوشیدن ',\n",
    "        ]\n",
    "num = 5\n",
    "kind = 'greedy'\n",
    "max_length = 20\n",
    "num_words = 2\n",
    "for sample in samples:\n",
    "    print('input : ', sample)\n",
    "    s_len = len(sample.split(' '))\n",
    "    predictions = predict_next(model_base, tokenizer_base, sample, kind , num, max_length)\n",
    "    for p in predictions:\n",
    "        preds = p['generated_text'].split()\n",
    "        print(' '.join(preds[s_len-1:s_len-1+num_words]))\n",
    "    print('.................................')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3c428-20ae-470c-80c3-c77638666b12",
   "metadata": {},
   "source": [
    "## Model with hidoctor/namnak dataset + alarabiyeh + lastsecond - 6 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b85f28-9480-4c36-bed2-5a1f5b992cd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  احتمال خطر سکته های \n",
      "فو به\n",
      "فو در\n",
      "فو یا\n",
      "فوبیا در\n",
      "فو سکته\n",
      ".................................\n",
      "input :  احتمال خطر سکته های م\n",
      "مگالوبلاستیک هم\n",
      "مگالوبلاستیک در\n",
      "ماحتمال و\n",
      "مگالوبلاستیک کم\n",
      "مگالوبلاستیک در\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس \n",
      "\n",
      "امتحان این\n",
      "از جمله\n",
      "و غلبه\n",
      "و افسردگی\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس ن\n",
      "ننوشیدن آب\n",
      "ننوشیدن آب\n",
      "ننوشیدن قهوه\n",
      "ننوشیدن قهوه\n",
      "ننوشیدن آب\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس نوشیدن \n",
      "آب فراوان\n",
      "یک فنجان\n",
      "چای کمرنگ\n",
      "چای سبز\n",
      "دمنوش آویشن\n",
      ".................................\n"
     ]
    }
   ],
   "source": [
    "# trained model result \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2_namnak_hidoctor/model_base_on_namnak_hidoctor_dataset_v2')\n",
    "\n",
    "samples = [ 'احتمال خطر سکته های ', \n",
    "         'احتمال خطر سکته های م',\n",
    "         'بهترین روش برای غلبه بر استرس ',\n",
    "         'بهترین روش برای غلبه بر استرس ن',\n",
    "         'بهترین روش برای غلبه بر استرس نوشیدن ',\n",
    "        ]\n",
    "num = 5\n",
    "kind = 'greedy'\n",
    "max_length = 20\n",
    "num_words = 2\n",
    "for sample in samples:\n",
    "    print('input : ', sample)\n",
    "    s_len = len(sample.split(' '))\n",
    "    predictions = predict_next(model.to('cpu'), tokenizer, sample, kind , num, max_length)\n",
    "    for p in predictions:\n",
    "        preds = p['generated_text'].split()\n",
    "#         print(p['generated_text'])\n",
    "        print(' '.join(preds[s_len-1:s_len-1+num_words]))\n",
    "    print('.................................')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bce406-40b9-44c8-b1ad-856f4d5b26e8",
   "metadata": {},
   "source": [
    "## Model with hidoctor + namnak dataset - 8 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f92ca1-1c5a-4533-8ea6-421df161d754",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  احتمال خطر سکته های \n",
      "کوپیک هم\n",
      "کمتر از\n",
      "فو در\n",
      "فو، در\n",
      "کوپلر و\n",
      ".................................\n",
      "input :  احتمال خطر سکته های م\n",
      "مگالوبلاستیک در\n",
      "مگالوبلاستیک در\n",
      "مگالوبلاستیک در\n",
      "مگالوبلاستیک بسیار\n",
      "مقلبی در\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس \n",
      "و نگرانی\n",
      "امتحان کنید\n",
      "تعطیلات\n",
      "و اضطراب،\n",
      "در تعطیلات\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس ن\n",
      "ننوشیدن آب\n",
      "ننوشیدن الکل\n",
      "ننوشیدن الکل.\n",
      "ننوشیدن الکل\n",
      "ننوشیدن الکل\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس نوشیدن \n",
      "آب گرم\n",
      "مایعات فراوان\n",
      "چای است.\n",
      "آبمیوه آناناس\n",
      "یک فنجان\n",
      ".................................\n"
     ]
    }
   ],
   "source": [
    "# trained model result \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model = GPT2LMHeadModel.from_pretrained('../gpt2_namnak_hidoctor/model_base_on_namnak_hidoctor_dataset_1')\n",
    "\n",
    "samples = [ 'احتمال خطر سکته های ', \n",
    "         'احتمال خطر سکته های م',\n",
    "         'بهترین روش برای غلبه بر استرس ',\n",
    "         'بهترین روش برای غلبه بر استرس ن',\n",
    "         'بهترین روش برای غلبه بر استرس نوشیدن ',\n",
    "        ]\n",
    "num = 5\n",
    "kind = 'greedy'\n",
    "max_length = 20\n",
    "num_words = 2\n",
    "for sample in samples:\n",
    "    print('input : ', sample)\n",
    "    s_len = len(sample.split(' '))\n",
    "    predictions = predict_next(model.to('cpu'), tokenizer, sample, kind , num, max_length)\n",
    "    for p in predictions:\n",
    "        preds = p['generated_text'].split()\n",
    "#         print(p['generated_text'])\n",
    "        print(' '.join(preds[s_len-1:s_len-1+num_words]))\n",
    "    print('.................................')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e53866-bfe3-4941-a54f-59d0de4e7e42",
   "metadata": {},
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85b6eb1-b11b-48fb-aef3-96beba759240",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(25001, 1024)\n",
       "    (wpe): Embedding(256, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=25001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained Model \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model.resize_token_embeddings(len(tokenizer))      \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fc52cd-4574-45a8-a5b4-57615c03ae86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make dataset\n",
    "dataset_sentences = load_dataset('dataset_sentences_v2')\n",
    "random.seed(10)\n",
    "dataset_sentences = random.sample(dataset_sentences, int(len(dataset_sentences)))\n",
    "dataset = GPT2Dataset(dataset_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafa2703-6f3a-41b2-bb25-f586160c51ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77182 training samples\n",
      "16540 validation samples\n",
      "16539 test samples\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.70 * len(dataset))\n",
    "test_size = int(0.15 * len(dataset))\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "batch_size = 14\n",
    "epochs = 3\n",
    "\n",
    "set_seed(555)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f'{train_size} training samples')\n",
    "print(f'{val_size} validation samples')\n",
    "print(f'{test_size} test samples')\n",
    "\n",
    "\n",
    "train_idx = list(range(0, int(0.8*len(train_dataset))))\n",
    "val_idx = list(range(0, int(0.8*len(val_dataset))))\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, val_idx)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9919e25-8cce-4191-b7c8-0f9711a9039f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from fastai.text.all import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ff8559-827d-46aa-9262-d7dd1aba821d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitter(model):\n",
    "    \"Split a GPT2 `model` in 3 groups for differential learning rates.\"\n",
    "    \n",
    "    # First layers group : decoder blocks from 0 to 3\n",
    "    modules = []\n",
    "    for i in range(4): modules.append(model.transformer.h[i])\n",
    "    groups = [nn.Sequential(*modules)]\n",
    "\n",
    "    # Second layers group : decoder blocks from 4 to 7\n",
    "    modules = []\n",
    "    for i in range(4,8,1): modules.append(model.transformer.h[i])\n",
    "    groups = L(groups + [nn.Sequential(*modules)])\n",
    "\n",
    "    # Third layers group : decoder blocks from 8 to 11\n",
    "    modules = []\n",
    "    for i in range(8,12,1): modules.append(model.transformer.h[i])\n",
    "    groups = L(groups + [nn.Sequential(*modules)])\n",
    "    \n",
    "    groups = L(groups + [nn.Sequential(model.transformer.wte,model.transformer.wpe,model.transformer.ln_f)])\n",
    "    \n",
    "    return groups.map(params)\n",
    "\n",
    "def plot_loss(train_losses, val_losses, epoch):\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    plt.plot(range(1,len(train_losses)+1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1,len(val_losses)+1), val_losses,label='Validation Loss')\n",
    "    plt.xlabel('epochs steps')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig('loss_plot_' + str(epoch)+ '.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7103a173-222b-4391-b9f2-476e9a0f6849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_groups = []\n",
    "learning_rates = [1e-4,2e-4,4e-4,5e-4]\n",
    "param_splitter = splitter(model)\n",
    "for i in range(0, len(param_splitter)):\n",
    "    parameters_of_group_layer = param_splitter[i]\n",
    "    for parameter in parameters_of_group_layer:\n",
    "        param_groups.append({'params': [parameter], 'lr': learning_rates[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0356b66-bd57-4feb-8b3b-f2a9fbd89144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/miniconda3/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "    \n",
    "learning_rate = 5e-4\n",
    "epsilon = 1e-8\n",
    "sample_every = 100\n",
    "\n",
    "optimizer = AdamW(param_groups,\n",
    "                  lr = learning_rate,\n",
    "                  eps = epsilon\n",
    "                )\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 10, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606a757-c766-4867-ab99-e73811828cbd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 6 ========\n",
      "  Batch   100  of  5,146. AVG TRAIN Loss: 0.9179818237772083. \n",
      "  Batch   100  of  5,146. AVG VAL Loss: 0.5495794613525636. \n",
      "  Batch   200  of  5,146. AVG TRAIN Loss: 0.5924027986194363. \n",
      "  Batch   200  of  5,146. AVG VAL Loss: 0.48923366679620006. \n",
      "  Batch   300  of  5,146. AVG TRAIN Loss: 0.4775616294422815. \n",
      "  Batch   300  of  5,146. AVG VAL Loss: 0.46555222976186117. \n",
      "  Batch   400  of  5,146. AVG TRAIN Loss: 0.41679004767440503. \n",
      "  Batch   400  of  5,146. AVG VAL Loss: 0.4615805326054989. \n",
      "  Batch   500  of  5,146. AVG TRAIN Loss: 0.38010999753684577. \n",
      "  Batch   500  of  5,146. AVG VAL Loss: 0.4532070746231598. \n",
      "  Batch   600  of  5,146. AVG TRAIN Loss: 0.35546352413053717. \n",
      "  Batch   600  of  5,146. AVG VAL Loss: 0.45083653066283663. \n",
      "  Batch   700  of  5,146. AVG TRAIN Loss: 0.3361253744780422. \n",
      "  Batch   700  of  5,146. AVG VAL Loss: 0.44862105799391394. \n",
      "  Batch   800  of  5,146. AVG TRAIN Loss: 0.32287257043795936. \n",
      "  Batch   800  of  5,146. AVG VAL Loss: 0.4462306279814038. \n",
      "  Batch   900  of  5,146. AVG TRAIN Loss: 0.31311899703099383. \n",
      "  Batch   900  of  5,146. AVG VAL Loss: 0.4447655960417615. \n",
      "  Batch 1,000  of  5,146. AVG TRAIN Loss: 0.30378423758468903. \n",
      "  Batch 1,000  of  5,146. AVG VAL Loss: 0.4434209201866567. \n",
      "  Batch 1,100  of  5,146. AVG TRAIN Loss: 0.2957249505819244. \n",
      "  Batch 1,100  of  5,146. AVG VAL Loss: 0.4417806232748308. \n",
      "  Batch 1,200  of  5,146. AVG TRAIN Loss: 0.28992551441097336. \n",
      "  Batch 1,200  of  5,146. AVG VAL Loss: 0.4404368198198939. \n",
      "  Batch 1,300  of  5,146. AVG TRAIN Loss: 0.2855152571423525. \n",
      "  Batch 1,300  of  5,146. AVG VAL Loss: 0.4410951081612709. \n",
      "  Batch 1,400  of  5,146. AVG TRAIN Loss: 0.28173310343684. \n",
      "  Batch 1,400  of  5,146. AVG VAL Loss: 0.43831129155965254. \n",
      "  Batch 1,500  of  5,146. AVG TRAIN Loss: 0.2780342414121402. \n",
      "  Batch 1,500  of  5,146. AVG VAL Loss: 0.4380760228671708. \n",
      "  Batch 1,600  of  5,146. AVG TRAIN Loss: 0.274133343177427. \n",
      "  Batch 1,600  of  5,146. AVG VAL Loss: 0.4378319042764354. \n",
      "  Batch 1,700  of  5,146. AVG TRAIN Loss: 0.27147961898911077. \n",
      "  Batch 1,700  of  5,146. AVG VAL Loss: 0.436446387742934. \n",
      "  Batch 1,800  of  5,146. AVG TRAIN Loss: 0.2689973950750096. \n",
      "  Batch 1,800  of  5,146. AVG VAL Loss: 0.4357468521092442. \n",
      "  Batch 1,900  of  5,146. AVG TRAIN Loss: 0.26635008445621977. \n",
      "  Batch 1,900  of  5,146. AVG VAL Loss: 0.43463370526852874. \n",
      "  Batch 2,000  of  5,146. AVG TRAIN Loss: 0.2645051217545336. \n",
      "  Batch 2,000  of  5,146. AVG VAL Loss: 0.43395702388215257. \n",
      "  Batch 2,100  of  5,146. AVG TRAIN Loss: 0.26194967112119627. \n",
      "  Batch 2,100  of  5,146. AVG VAL Loss: 0.4341253086938063. \n",
      "  Batch 2,200  of  5,146. AVG TRAIN Loss: 0.260108699007421. \n",
      "  Batch 2,200  of  5,146. AVG VAL Loss: 0.4334211735458452. \n",
      "  Batch 2,300  of  5,146. AVG TRAIN Loss: 0.25830056919501376. \n",
      "  Batch 2,300  of  5,146. AVG VAL Loss: 0.43274122604919585. \n",
      "  Batch 2,400  of  5,146. AVG TRAIN Loss: 0.256998919549459. \n",
      "  Batch 2,400  of  5,146. AVG VAL Loss: 0.4319843799281099. \n",
      "  Batch 2,500  of  5,146. AVG TRAIN Loss: 0.2553025907376965. \n",
      "  Batch 2,500  of  5,146. AVG VAL Loss: 0.4321714958221612. \n",
      "  Batch 2,600  of  5,146. AVG TRAIN Loss: 0.25386744826138363. \n",
      "  Batch 2,600  of  5,146. AVG VAL Loss: 0.43181870364375474. \n",
      "  Batch 2,700  of  5,146. AVG TRAIN Loss: 0.2524460910846021. \n",
      "  Batch 2,700  of  5,146. AVG VAL Loss: 0.43099043536542875. \n",
      "  Batch 2,800  of  5,146. AVG TRAIN Loss: 0.25124274043420175. \n",
      "  Batch 2,800  of  5,146. AVG VAL Loss: 0.43189017569218996. \n",
      "  Batch 2,900  of  5,146. AVG TRAIN Loss: 0.2502156368584232. \n",
      "  Batch 2,900  of  5,146. AVG VAL Loss: 0.4299412969422146. \n",
      "  Batch 3,000  of  5,146. AVG TRAIN Loss: 0.24928440714937097. \n",
      "  Batch 3,000  of  5,146. AVG VAL Loss: 0.43025337574380373. \n",
      "  Batch 3,100  of  5,146. AVG TRAIN Loss: 0.2483093888384417. \n",
      "  Batch 3,100  of  5,146. AVG VAL Loss: 0.4296789194034212. \n",
      "  Batch 3,200  of  5,146. AVG TRAIN Loss: 0.24759228384632573. \n",
      "  Batch 3,200  of  5,146. AVG VAL Loss: 0.43079531337332966. \n",
      "  Batch 3,300  of  5,146. AVG TRAIN Loss: 0.24657709714870532. \n",
      "  Batch 3,300  of  5,146. AVG VAL Loss: 0.4289804861044084. \n",
      "  Batch 3,400  of  5,146. AVG TRAIN Loss: 0.24570867385286962. \n",
      "  Batch 3,400  of  5,146. AVG VAL Loss: 0.4283123520132853. \n",
      "  Batch 3,500  of  5,146. AVG TRAIN Loss: 0.24549520623084647. \n",
      "  Batch 3,500  of  5,146. AVG VAL Loss: 0.4279185627280639. \n",
      "  Batch 3,600  of  5,146. AVG TRAIN Loss: 0.24465733992080163. \n",
      "  Batch 3,600  of  5,146. AVG VAL Loss: 0.42734568719418614. \n",
      "  Batch 3,700  of  5,146. AVG TRAIN Loss: 0.24387348446893356. \n",
      "  Batch 3,700  of  5,146. AVG VAL Loss: 0.4271766687536499. \n",
      "  Batch 3,800  of  5,146. AVG TRAIN Loss: 0.2430688371743161. \n",
      "  Batch 3,800  of  5,146. AVG VAL Loss: 0.4269188586706912. \n",
      "  Batch 3,900  of  5,146. AVG TRAIN Loss: 0.24262232776331125. \n",
      "  Batch 3,900  of  5,146. AVG VAL Loss: 0.426641160171354. \n",
      "  Batch 4,000  of  5,146. AVG TRAIN Loss: 0.24182971872312967. \n",
      "  Batch 4,000  of  5,146. AVG VAL Loss: 0.4267445981691668. \n",
      "  Batch 4,100  of  5,146. AVG TRAIN Loss: 0.24142525744413462. \n",
      "  Batch 4,100  of  5,146. AVG VAL Loss: 0.42617789291838354. \n",
      "  Batch 4,200  of  5,146. AVG TRAIN Loss: 0.2407774488785045. \n",
      "  Batch 4,200  of  5,146. AVG VAL Loss: 0.4258053355182395. \n",
      "  Batch 4,300  of  5,146. AVG TRAIN Loss: 0.24001527970348838. \n",
      "  Batch 4,300  of  5,146. AVG VAL Loss: 0.42527406313477706. \n",
      "  Batch 4,400  of  5,146. AVG TRAIN Loss: 0.23941735992192464. \n",
      "  Batch 4,400  of  5,146. AVG VAL Loss: 0.4244255249000957. \n",
      "  Batch 4,500  of  5,146. AVG TRAIN Loss: 0.23883730265146572. \n",
      "  Batch 4,500  of  5,146. AVG VAL Loss: 0.4244692661847619. \n",
      "  Batch 4,600  of  5,146. AVG TRAIN Loss: 0.23837442818678506. \n",
      "  Batch 4,600  of  5,146. AVG VAL Loss: 0.4239857452275638. \n",
      "  Batch 4,700  of  5,146. AVG TRAIN Loss: 0.23786308737485568. \n",
      "  Batch 4,700  of  5,146. AVG VAL Loss: 0.4235528964458113. \n",
      "  Batch 4,800  of  5,146. AVG TRAIN Loss: 0.23758979861629775. \n",
      "  Batch 4,800  of  5,146. AVG VAL Loss: 0.4240863566115458. \n",
      "  Batch 4,900  of  5,146. AVG TRAIN Loss: 0.23706455969750892. \n",
      "  Batch 4,900  of  5,146. AVG VAL Loss: 0.42347892898650785. \n",
      "  Batch 5,000  of  5,146. AVG TRAIN Loss: 0.2367284287916639. \n",
      "  Batch 5,000  of  5,146. AVG VAL Loss: 0.4228612701674537. \n",
      "  Batch 5,100  of  5,146. AVG TRAIN Loss: 0.2362795083271805. \n",
      "  Batch 5,100  of  5,146. AVG VAL Loss: 0.422441285951291. \n",
      "  Average training loss: 0.47\n",
      "  Perplexity: 1.60\n",
      "\n",
      "  Validation Loss : 0.42\n",
      "  Perplexity: 1.53\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'epoch': 1, 'Training Loss': 0.47212516036054014, 'Valid. Loss': 0.42252789613396496}]\n",
      "======== Epoch 2 / 6 ========\n",
      "  Batch   100  of  5,146. AVG TRAIN Loss: 0.20582789131025275. \n",
      "  Batch   100  of  5,146. AVG VAL Loss: 0.4225751749380869. \n",
      "  Batch   200  of  5,146. AVG TRAIN Loss: 0.2013736439507399. \n",
      "  Batch   200  of  5,146. AVG VAL Loss: 0.42218607460112323. \n",
      "  Batch   300  of  5,146. AVG TRAIN Loss: 0.20060658858365. \n",
      "  Batch   300  of  5,146. AVG VAL Loss: 0.42209988477169547. \n",
      "  Batch   400  of  5,146. AVG TRAIN Loss: 0.1994275351675074. \n",
      "  Batch   400  of  5,146. AVG VAL Loss: 0.4223654053206889. \n",
      "  Batch   500  of  5,146. AVG TRAIN Loss: 0.1988266542137502. \n",
      "  Batch   500  of  5,146. AVG VAL Loss: 0.4221068510848181. \n",
      "  Batch   600  of  5,146. AVG TRAIN Loss: 0.19865760575366298. \n",
      "  Batch   600  of  5,146. AVG VAL Loss: 0.42225835863450173. \n",
      "  Batch   700  of  5,146. AVG TRAIN Loss: 0.19735073906166237. \n",
      "  Batch   700  of  5,146. AVG VAL Loss: 0.42205677222686794. \n",
      "  Batch   800  of  5,146. AVG TRAIN Loss: 0.19750200982174176. \n",
      "  Batch   800  of  5,146. AVG VAL Loss: 0.42189925256525507. \n",
      "  Batch   900  of  5,146. AVG TRAIN Loss: 0.1980369867349703. \n",
      "  Batch   900  of  5,146. AVG VAL Loss: 0.422517957451229. \n",
      "  Batch 1,000  of  5,146. AVG TRAIN Loss: 0.19717487265596856. \n",
      "  Batch 1,000  of  5,146. AVG VAL Loss: 0.42264887548692637. \n",
      "  Batch 1,100  of  5,146. AVG TRAIN Loss: 0.19617576997297445. \n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, validation_dataloader):\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs  = model(b_input_ids, attention_mask = b_masks, labels=b_labels)\n",
    "            loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "        \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    return avg_val_loss\n",
    "    \n",
    "    \n",
    " # Gradual Layer Freezing(First layer)\n",
    "param_splitter = splitter(model)\n",
    "parameters_layer1 = param_splitter[0]\n",
    "for param in parameters_layer1:\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_stats = []\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    train_numder = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, labels=b_labels, attention_mask = b_masks, token_type_ids=None)\n",
    "\n",
    "        loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        train_numder += len(batch)\n",
    "\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "            train_losses.append(total_train_loss/train_numder)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. AVG TRAIN Loss: {:>5,}. '.format(step, len(train_dataloader), total_train_loss/train_numder))\n",
    "            model.eval()    \n",
    "            avg_val_loss = evaluate(model, validation_dataloader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. AVG VAL Loss: {:>5,}. '.format(step, len(train_dataloader), avg_val_loss))\n",
    "            model.train()\n",
    "                    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(f\"  Perplexity: {math.exp(avg_train_loss):.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "    model.eval()    \n",
    "    avg_val_loss = evaluate(model, validation_dataloader)\n",
    "    print(\"  Validation Loss : {0:.2f}\".format(avg_val_loss))\n",
    "    print(f\"  Perplexity: {math.exp(avg_val_loss):.2f}\")\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "        }\n",
    "    )\n",
    "    # Gradual Layer Freezing\n",
    "    if epoch_i == 0:\n",
    "        param_splitter = splitter(model)\n",
    "        parameters_layer2 = param_splitter[1]\n",
    "        for param in parameters_layer2:\n",
    "            param.requires_grad = False\n",
    "        plot_loss(train_losses, val_losses, epoch_i)\n",
    "        \n",
    "    if epoch_i == 1:\n",
    "        param_splitter = splitter(model)\n",
    "        parameters_layer3 = param_splitter[2]\n",
    "        for param in parameters_layer3:\n",
    "            param.requires_grad = False\n",
    "        plot_loss(train_losses, val_losses, epoch_i)\n",
    "    \n",
    "    if epoch_i == 2:\n",
    "        param_splitter = splitter(model)\n",
    "        parameters_layer4 = param_splitter[3]\n",
    "        for param in parameters_layer4:\n",
    "            param.requires_grad = False\n",
    "        plot_loss(train_losses, val_losses, epoch_i)\n",
    "            \n",
    "    model.save_pretrained('./gpt2_namnak_hidoctor/deep_model_on_namnak_hidoctor_dataset_epoch_i_' + str(epoch_i))\n",
    "    print(training_stats)\n",
    "    with open(\"training_stats\", \"wb\") as fp:   \n",
    "        pickle.dump(training_stats, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5ce1a40-57c7-4d28-a981-367de4723273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  احتمال خطر سکته های \n",
      "قلبی و\n",
      "قلبی را\n",
      "قلبی ناشی\n",
      "- مغزی\n",
      "پرترومبین هم\n",
      ".................................\n",
      "input :  احتمال خطر سکته های م\n",
      "مغذی برای\n",
      "مغزی را\n",
      "مگالوبلاستیک <<unk>sndof<unk>e\n",
      "مگالوبلاستیک در\n",
      "مغیر قلبی\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس \n",
      "خود چه\n",
      "تان در\n",
      "یا جلوگیری\n",
      "ی که\n",
      "‌های روزانه\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس ن\n",
      "ننشینین و\n",
      "ننشینین و\n",
      "ننشینید <<unk>sndof<unk>e\n",
      "ننوشیدن الکل\n",
      "نپوشیدن لباسهای\n",
      ".................................\n",
      "input :  بهترین روش برای غلبه بر استرس نوشیدن \n",
      "یک فنجان\n",
      "چای سیاه\n",
      "یک فنجان\n",
      "آب است\n",
      "چای و\n",
      ".................................\n"
     ]
    }
   ],
   "source": [
    "# trained model result \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model = GPT2LMHeadModel.from_pretrained('./gpt2_namnak_hidoctor/deep_model_on_namnak_hidoctor_dataset_epoch_i_1')\n",
    "\n",
    "samples = [ 'احتمال خطر سکته های ', \n",
    "         'احتمال خطر سکته های م',\n",
    "         'بهترین روش برای غلبه بر استرس ',\n",
    "         'بهترین روش برای غلبه بر استرس ن',\n",
    "         'بهترین روش برای غلبه بر استرس نوشیدن ',\n",
    "        ]\n",
    "num = 5\n",
    "kind = 'greedy'\n",
    "max_length = 20\n",
    "num_words = 2\n",
    "for sample in samples:\n",
    "    print('input : ', sample)\n",
    "    s_len = len(sample.split(' '))\n",
    "    predictions = predict_next(model.to('cpu'), tokenizer, sample, kind , num, max_length)\n",
    "    for p in predictions:\n",
    "        preds = p['generated_text'].split()\n",
    "#         print(p['generated_text'])\n",
    "        print(' '.join(preds[s_len-1:s_len-1+num_words]))\n",
    "    print('.................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565dbc8-d47b-4ba5-a27a-bf2dfbc41ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
