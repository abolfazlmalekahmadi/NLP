{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a29a7431",
   "metadata": {},
   "source": [
    "# üü¢ Data gather from Git Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd55a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:32<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_url = \"https://github.com/language-ml/course-nlp-ir-1-text-exploring/raw/main/exploring-datasets/health/\"  # Replace with your JSON URL\n",
    "file_names = ['hidoctor-1.json', 'hidoctor-2.json', 'hidoctor-3.json', 'hidoctor-4.json', 'hidoctor-5.json',\n",
    "              'namnak-1.json', 'namnak-2.json', 'namnak-3.json', 'namnak-4.json', 'namnak-5.json']\n",
    "url_list = [base_url + file_name for file_name in file_names]\n",
    "all_paragraphs = []\n",
    "\n",
    "for url in tqdm(url_list):\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    for dict_ in data:\n",
    "        all_paragraphs += dict_['paragraphs']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ee5f239",
   "metadata": {},
   "source": [
    "# üü¢ No need for smoothing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f67a49f",
   "metadata": {},
   "source": [
    "# üü¢ pattern = r'\\b' + str_ngram + r'\\b' # it's so importatnt because of preventing sub simple words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47977f40",
   "metadata": {},
   "source": [
    "ÿ≥ÿ± ŸáŸÖ€åŸÜ €±€¥ ÿ≥ÿßÿπÿ™ ÿ®ÿ±ŸÜÿßŸÖŸá ÿßŸÑ⁄©€å ÿ±ÿßŸÜ ÿ¥ÿØüü¢ .................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import sent_tokenize\n",
    "from nltk import ngrams\n",
    "import regex as re\n",
    "from hazm import Normalizer\n",
    "from hazm import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Generator_ngram:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        '''\n",
    "        n for ngram\n",
    "        '''\n",
    "        self.n = n\n",
    "    \n",
    "    def __preprocess(self, all_paragraphs):\n",
    "        # Text preprocessing\n",
    "        \n",
    "        # Extract sentences from paragraphs :\n",
    "        sentences_list = []\n",
    "        for paragraph in tqdm(all_paragraphs, desc = 'Sentences tokenization'):\n",
    "            sentences_list += sent_tokenize(paragraph)\n",
    "        \n",
    "        # Normalize + remove extra denotations like :\n",
    "        normalized_sentences_list = []\n",
    "        normalizer = Normalizer()\n",
    "        for sentence in tqdm(sentences_list, desc = 'Normalization'):\n",
    "            normalized_sentence = re.sub('[:,ÿå.<>/!@#$%~{}();¬ª¬´‚Ä¶‚Äú‚Äù\"ÿõÿü‚óä‚ô¶‚Äì\\*\\+_\\^]', ' ', sentence)\n",
    "            normalized_sentence = normalizer.normalize(normalized_sentence)\n",
    "            normalized_sentences_list.append(normalized_sentence)        \n",
    "            \n",
    "        # Extract tokens from normalized sentences\n",
    "        self.all_tokens = [] # <s> and </s> tags are added\n",
    "        for sentence in tqdm(normalized_sentences_list, desc = 'Tokenization'):\n",
    "            self.all_tokens.append(\"<s>\")\n",
    "            temp_tokens = word_tokenize(sentence)\n",
    "            # ÿπŸÑÿßŸÖÿ™ Ÿáÿß€å ÿÆ€åŸÑ€å ÿ≤€åÿßÿØ€å Ÿàÿ¨ŸàÿØ ÿØÿßÿ±Ÿá Ÿà Ÿæ€åÿØÿß ⁄©ÿ±ÿØŸÜ ŸáŸÖÿ¥ŸàŸÜ ÿ≥ÿÆÿ™Ÿá ŸÅ⁄©ÿ± ⁄©ŸÜŸÖ ÿ™ŸÜŸáÿß ⁄©ŸÑŸÖŸá ÿ™⁄© ÿ≠ÿ±ŸÅ€å ÿ™Ÿà€å ŸÅÿßÿ±ÿ≥€å 'Ÿà' ÿ®ÿßÿ¥Ÿá \n",
    "            # Ÿæÿ≥ ÿ®Ÿá ÿ∫€åÿ± ÿßÿ≤ 'Ÿà' ŸáŸÖŸá ÿ™Ÿà⁄©ŸÜ Ÿáÿß€å ÿ™⁄© ÿ≠ÿ±ŸÅ€å ÿ±Ÿà ÿ≠ÿ∞ŸÅ ⁄©ŸÜ€åŸÖ\n",
    "            for token in temp_tokens:\n",
    "                if len(token) > 1 or token == 'Ÿà':\n",
    "                    self.all_tokens.append(token)\n",
    "            self.all_tokens.append(\"</s>\")\n",
    "\n",
    "        self.remaked_corpus = ' '.join(self.all_tokens) # use this for search and count\n",
    "        \n",
    "        \n",
    "    def fit(self, input_corpus):\n",
    "        '''\n",
    "        input_corpus is a list(each item should be a string) of paragraphs.\n",
    "        '''\n",
    "        \n",
    "        self.__preprocess(input_corpus)\n",
    "        \n",
    "        df_ngrams = pd.DataFrame(columns = ['ngram', 'count'])\n",
    "        df_indx = -1\n",
    "        for ngram in tqdm(set(ngrams(self.all_tokens, self.n)), desc = f'{self.n}_grams calculation'): # use set to ignore repeated ones\n",
    "            if '<s>' not in ngram and '</s>' not in ngram:\n",
    "                str_ngram = ' '.join(ngram)\n",
    "                try :\n",
    "                    pattern = r'\\b' + str_ngram + r'\\b' # it's so importatnt because of preventing sub simple words\n",
    "                    count = len(re.findall(pattern, self.remaked_corpus))\n",
    "                    if count > 0:\n",
    "                        df_indx += 1\n",
    "                        df_ngrams.loc[df_indx, 'ngram'] = str_ngram\n",
    "                        df_ngrams.loc[df_indx, 'count'] = count\n",
    "                except:\n",
    "                    print(\"Error at :\")\n",
    "                    print(str_ngram)\n",
    "        \n",
    "        self.df_ngrams = df_ngrams   \n",
    "        self.df_ngrams.sort_values(by = 'count', ascending = False, inplace = True)\n",
    "            \n",
    "    def set_pre_probs(self, ngram_file_name):\n",
    "        '''\n",
    "        set pre calculated ngrams from file.\n",
    "        ngram_file is a .csv with \"ngram\", \"count\" as columns\n",
    "        '''\n",
    "        self.df_ngrams = pd.read_csv(ngram_file_name)\n",
    "        self.df_ngrams.sort_values(by = 'count', ascending = False, inplace = True)\n",
    "        \n",
    "    \n",
    "    def generate(self,input_text, top_n):\n",
    "        '''\n",
    "        input_text : user input text\n",
    "        top_n : top n words to show\n",
    "        '''\n",
    "        input_text = re.sub('[:,ÿå.<>/!@#$%~{}();¬ª¬´‚Ä¶‚Äú‚Äù\"ÿõÿü‚óä‚ô¶‚Äì\\*\\+_\\^]', ' ', input_text)\n",
    "        normalizer = Normalizer(correct_spacing = False)\n",
    "        input_text = normalizer.normalize(input_text)\n",
    "\n",
    "        input_text_tokens = input_text.split()\n",
    "        last_word = input_text_tokens[-1] # no worries with space --> split doesn't count last space\n",
    "            \n",
    "        last_incomplete = True\n",
    "        if input_text[-1] == ' ': # So the last word is complete\n",
    "            last_incomplete = False\n",
    "\n",
    "        if not last_incomplete: #complete\n",
    "            if (self.n -1 > len(input_text_tokens)):\n",
    "                raise Exception(f\"input text must be longer than {self.n} words\")\n",
    "        else: #incomplete\n",
    "            if (self.n > len(input_text_tokens)):\n",
    "                raise Exception(f\"input text must be longer than {self.n} words\")\n",
    "                \n",
    "                \n",
    "        df_output = self.df_ngrams.copy()\n",
    "        if not last_incomplete: # last complete\n",
    "            if self.n == 1:\n",
    "                df_output['ngram'] = df_output['ngram'].str.split().str[-1]\n",
    "                return df_output.head(top_n).values\n",
    "            else:\n",
    "                input_ngram_list = input_text_tokens[-(self.n-1):]\n",
    "                str_input_ngram = ' '.join(input_ngram_list)\n",
    "                df_output = df_output[df_output['ngram'].str.startswith(str_input_ngram + \" \")]\n",
    "                df_output['ngram'] = df_output['ngram'].str.split().str[-1]\n",
    "                return df_output.head(top_n).values\n",
    "            \n",
    "        else: # incomplete\n",
    "            if self.n == 1:\n",
    "                df_output = df_output[df_output['ngram'].str.startswith(last_word)]\n",
    "                df_output['ngram'] = df_output['ngram'].str.split().str[-1]\n",
    "                return df_output.head(top_n).values\n",
    "            else:\n",
    "                input_ngram_list = input_text_tokens[-self.n:] # no need to subtract by 1 because last incomplete\n",
    "                str_input_ngram = ' '.join(input_ngram_list)\n",
    "                df_output = df_output[df_output['ngram'].str.startswith(str_input_ngram)]\n",
    "                df_output['ngram'] = df_output['ngram'].str.split().str[-1]\n",
    "                return df_output.head(top_n).values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac1070ee",
   "metadata": {},
   "source": [
    "# 1 gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd7160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ŸÖÿ∫ÿ≤', 1395],\n",
       "       ['ŸÖÿ∫ÿ≤€å', 565],\n",
       "       ['ŸÖÿ∫ÿ≤Ÿáÿß', 35],\n",
       "       ['ŸÖÿ∫ÿ≤ÿ™ÿßŸÜ', 22],\n",
       "       ['ŸÖÿ∫ÿ≤Ÿáÿß€å', 11]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator_ngram(1)\n",
    "model.set_pre_probs('1_gram_probs.csv') # read from trained & saved ngrams\n",
    "text =input()\n",
    "model.generate(text, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "335f7c82",
   "metadata": {},
   "source": [
    "# 2 gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70068b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Ÿà', 94],\n",
       "       ['ÿ®ÿßÿØÿßŸÖ', 87],\n",
       "       ['ŸÖÿßÿØÿ±', 77],\n",
       "       ['⁄ØÿßŸà', 50],\n",
       "       ['ÿ±ÿß', 46]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator_ngram(2)\n",
    "model.set_pre_probs('2_gram_probs.csv') # read from trained & saved ngrams\n",
    "text = input()\n",
    "model.generate(text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßŸÅÿ≤ÿß€åÿ¥ ŸÜÿ±ÿÆ \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['ŸÖÿ±⁄Ø', 4],\n",
       "       ['ÿ®ÿßŸÑÿß€å€å', 3],\n",
       "       ['ÿ≥ŸàÿÆÿ™', 3],\n",
       "       ['ÿ®ŸÇÿß€å', 2],\n",
       "       ['⁄ÜÿßŸÇ€å', 2]], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Generator_ngram(2)\n",
    "model.set_pre_probs('2_gram_probs.csv') # read from trained & saved ngrams\n",
    "text = input()\n",
    "model.generate(text, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f2297d4",
   "metadata": {},
   "source": [
    "<h1>\n",
    "ÿß⁄ØŸá ÿ™ŸàŸÜÿ≥ÿ™€å ÿ±Ÿà€å\n",
    "<br>\n",
    "n = 3 \n",
    "<br>\n",
    "ŸáŸÖ ÿ±ÿßŸÜ ÿ®⁄Ø€åÿ±\n",
    "    </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58995/58995 [00:00<00:00, 180921.81it/s]\n",
      "Normalization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:04<00:00, 20177.87it/s]\n",
      "Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:01<00:00, 65381.13it/s]\n",
      "1_grams calculation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39694/39694 [05:28<00:00, 120.80it/s]\n",
      "Sentences tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58995/58995 [00:00<00:00, 183944.43it/s]\n",
      "Normalization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:04<00:00, 20053.69it/s]\n",
      "Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:01<00:00, 73008.99it/s]\n",
      "2_grams calculation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 454110/454110 [1:56:23<00:00, 65.03it/s]\n",
      "Sentences tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58995/58995 [00:00<00:00, 174801.19it/s]\n",
      "Normalization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:04<00:00, 19837.49it/s]\n",
      "Tokenization: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95636/95636 [00:01<00:00, 72146.14it/s]\n",
      "3_grams calculation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1015452/1015452 [6:04:43<00:00, 46.40it/s] \n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3]:\n",
    "    model = Generator_ngram(n)\n",
    "    model.fit(all_paragraphs)\n",
    "    model.df_ngrams.to_csv(f'{n}_gram_probs.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48715893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
